"""ç³»ç»Ÿè¿ç»´é¢æ¿ï¼šæ‰‹åŠ¨æ§åˆ¶åç«¯å„é¡¹åå°æµç¨‹ã€‚"""
from __future__ import annotations

from typing import Any, Dict

import pandas as pd
import streamlit as st  # type: ignore[import]

from utils import call_backend, get_json


def _render_status_panel(status: Dict[str, Any]) -> None:
    st.markdown("### ğŸ“ˆ å½“å‰è¿è¡Œæ€")
    ingestion = status.get("ingestion", {})
    loop_status = status.get("incremental_loop", {})

    col_a, col_b, col_c = st.columns(3)
    with col_a:
        st.metric(
            "æ•°æ®æ‘„å–",
            "è¿è¡Œä¸­" if ingestion.get("running") else "å·²åœæ­¢",
            help="SystemController æ±‡æŠ¥çš„æ•°æ®æ‘„å–çŠ¶æ€",
        )
    with col_b:
        st.metric(
            "å¾…å¤„ç†äº‹ä»¶",
            ingestion.get("pending_events", "N/A"),
            help="å½“å‰æ’é˜Ÿç­‰å¾…å¢é‡å­¦ä¹ çš„äº‹ä»¶æ•°é‡",
        )
    with col_c:
        st.metric(
            "å¢é‡å¾ªç¯",
            "è¿è¡Œä¸­" if loop_status.get("running") else "å·²åœæ­¢",
            help="åå°å¢é‡è®­ç»ƒå¾ªç¯çŠ¶æ€",
        )

    extra_cols = st.columns(3)
    with extra_cols[0]:
        st.metric(
            "æ‘„å–å¼€å§‹æ—¶é—´",
            ingestion.get("started_at", "-"),
            help="æœ€è¿‘ä¸€æ¬¡å¯åŠ¨æ‘„å–çš„æ—¶é—´æˆ³",
        )
    with extra_cols[1]:
        st.metric(
            "æœ€åäº‹ä»¶æ—¶é—´",
            ingestion.get("last_event_at", "-"),
            help="äº‹ä»¶é˜Ÿåˆ—æœ€è¿‘ä¸€æ¬¡å…¥é˜Ÿæ—¶é—´",
        )
    with extra_cols[2]:
        st.metric(
            "å¢é‡æœ€åæ‰¹",
            loop_status.get("last_batch_at", "-"),
            help="å¢é‡å­¦ä¹ æœ€è¿‘ä¸€æ¬¡æ¶ˆè´¹äº‹ä»¶çš„æ—¶é—´æˆ³",
        )

    history = status.get("history", [])
    if history:
        st.markdown("#### ğŸ“ æœ€è¿‘æ“ä½œè®°å½•")
        df = pd.DataFrame(history)
        st.dataframe(df, use_container_width=True)
    else:
        st.info("æš‚æ— å†å²æ“ä½œè®°å½•ã€‚")


@st.cache_data(ttl=5, show_spinner=False)
def _fetch_health_cached() -> Dict[str, Any]:
    data = get_json("/health", timeout=8)
    return data or {}


@st.cache_data(ttl=5, show_spinner=False)
def _fetch_status_cached() -> Dict[str, Any] | None:
    return get_json("/api/v1/operations/status", timeout=8)


def _refresh_status_panel(placeholder) -> Dict[str, Any] | None:
    _fetch_status_cached.clear()
    latest = _fetch_status_cached()
    if latest:
        with placeholder.container():
            _render_status_panel(latest)
    else:
        placeholder.empty()
    return latest


st.title("ç³»ç»Ÿè¿ç»´é¢æ¿")
st.caption("æ‰‹åŠ¨æ§åˆ¶æ•°æ®æ‘„å–ã€å¢é‡å­¦ä¹ ä¸æ¨¡å‹è®­ç»ƒæµç¨‹")

health_info = _fetch_health_cached()
if health_info:
    device_mode = health_info.get("device_mode", "-")
    device_str = health_info.get("device", "-")
    mode_label = "GPU" if device_mode.lower() == "gpu" else ("CPU" if device_mode.lower() == "cpu" else device_mode)
    st.markdown(f"**è®¾å¤‡æ¨¡å¼**ï¼š{mode_label} Â· è®¾å¤‡ï¼š`{device_str}`")

status_placeholder = st.empty()
status_data = _fetch_status_cached()
if status_data:
    with status_placeholder.container():
        _render_status_panel(status_data)
else:
    st.warning("æœªèƒ½åŠ è½½ç³»ç»ŸçŠ¶æ€ï¼Œè¯·ç¨åé‡è¯•ã€‚")

if st.button("åˆ·æ–°çŠ¶æ€æ¦‚è§ˆ", type="secondary"):
    status_data = _refresh_status_panel(status_placeholder)
    if not status_data:
        st.warning("æœªèƒ½åŠ è½½ç³»ç»ŸçŠ¶æ€ï¼Œè¯·ç¨åé‡è¯•ã€‚")

st.markdown("### âš™ï¸ ä»»åŠ¡æ§åˆ¶")
col1, col2 = st.columns(2)
with col1:
    if st.button("å¯åŠ¨æ•°æ®æ‘„å–", type="primary"):
        result = call_backend(
            "/api/v1/operations/ingestion/start",
            method="POST",
            spinner="æ­£åœ¨å¯åŠ¨æ•°æ®æ‘„å–...",
            timeout=60,
        )
        if result is not None:
            st.success(result.get("message", "æ•°æ®æ‘„å–å·²å¯åŠ¨"))
            status_snapshot = result.get("status")
            if status_snapshot:
                st.json(status_snapshot)
            _refresh_status_panel(status_placeholder)
with col2:
    if st.button("åœæ­¢æ•°æ®æ‘„å–", type="secondary"):
        result = call_backend(
            "/api/v1/operations/ingestion/stop",
            method="POST",
            spinner="æ­£åœ¨åœæ­¢æ•°æ®æ‘„å–...",
            timeout=60,
        )
        if result is not None:
            st.success(result.get("message", "æ•°æ®æ‘„å–å·²åœæ­¢"))
            status_snapshot = result.get("status")
            if status_snapshot:
                st.json(status_snapshot)
            _refresh_status_panel(status_placeholder)

col3, col4 = st.columns(2)
with col3:
    if st.button("å¯åŠ¨å¢é‡å¾ªç¯", type="primary"):
        result = call_backend(
            "/api/v1/operations/incremental/start",
            method="POST",
            spinner="æ­£åœ¨å¯åŠ¨å¢é‡å¾ªç¯...",
            timeout=60,
        )
        if result is not None:
            st.success(result.get("message", "å¢é‡å¾ªç¯å·²å¯åŠ¨"))
            loop_snapshot = result.get("status")
            if loop_snapshot:
                st.json(loop_snapshot)
            _refresh_status_panel(status_placeholder)
with col4:
    if st.button("åœæ­¢å¢é‡å¾ªç¯", type="secondary"):
        result = call_backend(
            "/api/v1/operations/incremental/stop",
            method="POST",
            spinner="æ­£åœ¨åœæ­¢å¢é‡å¾ªç¯...",
            timeout=60,
        )
        if result is not None:
            st.success(result.get("message", "å¢é‡å¾ªç¯å·²åœæ­¢"))
            loop_snapshot = result.get("status")
            if loop_snapshot:
                st.json(loop_snapshot)
            _refresh_status_panel(status_placeholder)

st.markdown("### ğŸ§  èåˆæ¨¡å‹è®­ç»ƒ")
with st.form("fusion_train_form"):
    sample_size = st.number_input("é‡‡æ ·ç”¨æˆ·æ•°", min_value=32, max_value=10000, value=256, step=32)
    epochs = st.number_input("è®­ç»ƒè½®æ¬¡", min_value=1, max_value=200, value=3, step=1)
    lr = st.number_input("å­¦ä¹ ç‡", min_value=1e-5, max_value=1.0, value=1e-3, step=1e-4, format="%.5f")
    batch_size = st.number_input("æ‰¹å¤§å°", min_value=8, max_value=1024, value=64, step=8)
    submitted = st.form_submit_button("å¼€å§‹è®­ç»ƒ", type="primary")
    if submitted:
        result = call_backend(
            "/api/v1/operations/fusion/train",
            method="POST",
            payload={
                "sample_size": int(sample_size),
                "epochs": int(epochs),
                "lr": float(lr),
                "batch_size": int(batch_size),
            },
            spinner="èåˆæ ¸å¿ƒè®­ç»ƒè¿›è¡Œä¸­...",
            timeout=600,
        )
        if result is not None:
            st.success(result.get("message", "è®­ç»ƒå·²å®Œæˆ"))
            st.json(result.get("metrics", result))
            _refresh_status_panel(status_placeholder)

st.markdown("### ğŸ•¸ï¸ HGT å›¾è¡¨å¾è®­ç»ƒ")
with st.form("hgt_training_form"):
    st.caption("é…ç½®é®è”½æ¯”ä¾‹ä¸è®­ç»ƒå‚æ•°ï¼Œå‰ç«¯ä¸€é”®è§¦å‘åç«¯ HGT è®­ç»ƒä¸è¯„ä¼°æµç¨‹ã€‚")
    hgt_epochs = st.number_input("HGT è®­ç»ƒè½®æ¬¡", min_value=1, max_value=200, value=5, step=1)
    train_ratio = st.slider("è®­ç»ƒé›†æ¯”ä¾‹", min_value=0.5, max_value=0.95, value=0.8, step=0.05)
    max_val_ratio = max(0.0, min(0.4, 0.99 - float(train_ratio)))
    default_val_ratio = 0.1 if 0.1 <= max_val_ratio else round(max_val_ratio, 2)
    val_ratio = st.slider(
        "éªŒè¯é›†æ¯”ä¾‹",
        min_value=0.0,
        max_value=max_val_ratio,
        value=default_val_ratio,
        step=0.01,
    )
    if train_ratio + val_ratio >= 1.0:
        st.warning("è®­ç»ƒé›†ä¸éªŒè¯é›†æ¯”ä¾‹ä¹‹å’Œéœ€å°äº 1.0ï¼Œå½“å‰è®¾ç½®å°†ä¸ä¼šç•™ä¸‹æµ‹è¯•é›†ã€‚")
    negative_ratio = st.slider("è´Ÿæ ·æœ¬å€æ•°", min_value=0.0, max_value=5.0, value=1.0, step=0.1)
    temperature = st.slider("å¯¹æ¯”æŸå¤±æ¸©åº¦", min_value=0.05, max_value=1.0, value=0.2, step=0.05)
    learning_rate = st.number_input(
        "è‡ªå®šä¹‰å­¦ä¹ ç‡ (å¯é€‰)",
        min_value=0.0,
        max_value=1.0,
        value=0.0,
        step=1e-4,
        format="%.5f",
        help="è®¾ç½®ä¸º 0 è¡¨ç¤ºæ²¿ç”¨å½“å‰ä¼˜åŒ–å™¨å­¦ä¹ ç‡",
    )
    seed = st.text_input("éšæœºç§å­ (å¯é€‰)", value="")
    
    # è®­ç»ƒæ¨¡å¼é…ç½®
    st.markdown("#### ğŸ¯ è®­ç»ƒæ¨¡å¼é€‰æ‹©")
    training_mode = st.radio(
        "é€‰æ‹©è®­ç»ƒæ•°æ®æº",
        options=["å®Œæ•´å›¾è®­ç»ƒ", "METIS é‡‡æ ·å­å›¾è®­ç»ƒ"],
        index=0,
        help="å®Œæ•´å›¾è®­ç»ƒä½¿ç”¨æ‰€æœ‰æ•°æ®ï¼ŒMETIS é‡‡æ ·è®­ç»ƒä½¿ç”¨å›¾åˆ†å‰²åçš„å­å›¾",
    )
    
    # METIS é‡‡æ ·é…ç½®ï¼ˆä»…åœ¨é€‰æ‹© METIS æ¨¡å¼æ—¶æ˜¾ç¤ºï¼‰
    if training_mode == "METIS é‡‡æ ·å­å›¾è®­ç»ƒ":
        st.markdown("##### METIS é‡‡æ ·å‚æ•°é…ç½®")
        metis_num_parts = st.slider(
            "åˆ†åŒºæ•°é‡",
            min_value=2,
            max_value=20,
            value=4,
            step=1,
            help="å°†å›¾åˆ†å‰²ä¸ºå¤šå°‘ä¸ªåˆ†åŒºï¼Œåˆ†åŒºæ•°è¶Šå¤šï¼Œæ¯ä¸ªå­å›¾è¶Šå°",
        )
        metis_imbalance_factor = st.slider(
            "ä¸å¹³è¡¡å› å­",
            min_value=0.0,
            max_value=1.0,
            value=0.01,
            step=0.01,
            help="å…è®¸åˆ†åŒºå¤§å°ä¸å‡è¡¡çš„ç¨‹åº¦ï¼Œ0 è¡¨ç¤ºä¸¥æ ¼å‡è¡¡",
        )
        metis_recursive = st.checkbox(
            "ä½¿ç”¨é€’å½’äºŒåˆ†æ³•",
            value=True,
            help="é€’å½’äºŒåˆ†æ³•é€šå¸¸èƒ½è·å¾—æ›´å¥½çš„åˆ†åŒºè´¨é‡",
        )
        metis_seed_input = st.text_input(
            "METIS éšæœºç§å­ (å¯é€‰)",
            value="",
            help="ç”¨äºå¯é‡å¤çš„åˆ†åŒºç»“æœ",
        )
        metis_partition_id_input = st.text_input(
            "æŒ‡å®šåˆ†åŒº ID (å¯é€‰)",
            value="",
            help=f"æŒ‡å®šä½¿ç”¨å“ªä¸ªåˆ†åŒºï¼ˆ0-{metis_num_parts-1}ï¼‰ï¼Œç•™ç©ºåˆ™éšæœºé€‰æ‹©",
        )
    
    submitted_hgt = st.form_submit_button("è¿è¡Œ HGT è®­ç»ƒ", type="primary")
    if submitted_hgt:
        payload: Dict[str, Any] = {
            "epochs": int(hgt_epochs),
            "train_ratio": float(train_ratio),
            "val_ratio": float(val_ratio),
            "negative_ratio": float(negative_ratio),
            "temperature": float(temperature),
        }
        if learning_rate > 0:
            payload["learning_rate"] = float(learning_rate)
        seed = seed.strip()
        if seed:
            try:
                payload["seed"] = int(seed)
            except ValueError:
                st.warning("éšæœºç§å­éœ€ä¸ºæ•´æ•°ï¼Œå°†å¿½ç•¥è¯¥è¾“å…¥ã€‚")
        
        # æ·»åŠ è®­ç»ƒæ¨¡å¼é…ç½®
        if training_mode == "å®Œæ•´å›¾è®­ç»ƒ":
            payload["training_mode"] = "full_graph"
        else:
            payload["training_mode"] = "metis_sampling"
            payload["metis_num_parts"] = int(metis_num_parts)
            payload["metis_imbalance_factor"] = float(metis_imbalance_factor)
            payload["metis_recursive"] = bool(metis_recursive)
            
            metis_seed = metis_seed_input.strip()
            if metis_seed:
                try:
                    payload["metis_seed"] = int(metis_seed)
                except ValueError:
                    st.warning("METIS éšæœºç§å­éœ€ä¸ºæ•´æ•°ï¼Œå°†å¿½ç•¥è¯¥è¾“å…¥ã€‚")
            
            metis_partition_id = metis_partition_id_input.strip()
            if metis_partition_id:
                try:
                    pid = int(metis_partition_id)
                    if 0 <= pid < metis_num_parts:
                        payload["metis_partition_id"] = pid
                    else:
                        st.warning(f"åˆ†åŒº ID å¿…é¡»åœ¨ 0-{metis_num_parts-1} ä¹‹é—´ï¼Œå°†ä½¿ç”¨éšæœºé€‰æ‹©ã€‚")
                except ValueError:
                    st.warning("åˆ†åŒº ID éœ€ä¸ºæ•´æ•°ï¼Œå°†ä½¿ç”¨éšæœºé€‰æ‹©ã€‚")
        
        result = call_backend(
            "/api/v1/operations/training/hgt",
            method="POST",
            payload=payload,
            spinner="HGT è®­ç»ƒä¸è¯„ä¼°è¿›è¡Œä¸­...",
            timeout=600,
        )
        if result is not None:
            st.success(result.get("message", "HGT è®­ç»ƒå®Œæˆ"))
            summary = result.get("summary", result)
            _refresh_status_panel(status_placeholder)
            
            # æ˜¾ç¤ºå¤§å›¾ç»Ÿè®¡ä¿¡æ¯
            if "graph_statistics" in summary:
                st.markdown("#### ğŸ“Š å®Œæ•´å¤§å›¾ç»Ÿè®¡")
                graph_stats = summary["graph_statistics"]
                
                col_g1, col_g2 = st.columns(2)
                with col_g1:
                    st.metric("æ€»èŠ‚ç‚¹æ•°", graph_stats.get("total_nodes", "-"))
                with col_g2:
                    st.metric("æ€»è¾¹æ•°", graph_stats.get("total_edges", "-"))
                
                st.markdown("##### èŠ‚ç‚¹ç»Ÿè®¡")
                node_counts = graph_stats.get("node_counts", {})
                if node_counts:
                    node_df = pd.DataFrame({
                        "èŠ‚ç‚¹ç±»å‹": list(node_counts.keys()),
                        "èŠ‚ç‚¹æ•°": list(node_counts.values()),
                    })
                    st.dataframe(node_df, use_container_width=True)
                
                st.markdown("##### è¾¹ç»Ÿè®¡")
                edge_counts = graph_stats.get("edge_counts", {})
                if edge_counts:
                    edge_df = pd.DataFrame({
                        "è¾¹ç±»å‹": list(edge_counts.keys()),
                        "è¾¹æ•°": list(edge_counts.values()),
                    })
                    st.dataframe(edge_df, use_container_width=True)
                
                # æ˜¾ç¤ºä¿å­˜è·¯å¾„
                if "graph_save_path" in summary:
                    st.info(f"å®Œæ•´å¤§å›¾å·²ä¿å­˜è‡³: {summary['graph_save_path']}")
            
            # æ˜¾ç¤ºé‡‡æ ·ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            if "sampling_stats" in summary:
                st.markdown("#### ğŸ“Š METIS é‡‡æ ·ç»Ÿè®¡")
                stats = summary["sampling_stats"]
                
                col_s1, col_s2, col_s3 = st.columns(3)
                with col_s1:
                    st.metric("é€‰ä¸­åˆ†åŒº", stats.get("selected_partition", "-"))
                with col_s2:
                    st.metric("è¾¹åˆ‡å‰²æ•°", stats.get("edge_cut", "-"))
                with col_s3:
                    original_nodes_total = sum(stats.get("original_nodes", {}).values())
                    sampled_nodes_total = sum(stats.get("sampled_nodes", {}).values())
                    sampling_ratio = sampled_nodes_total / original_nodes_total if original_nodes_total > 0 else 0
                    st.metric("é‡‡æ ·æ¯”ä¾‹", f"{sampling_ratio:.2%}")
                
                st.markdown("##### èŠ‚ç‚¹ç»Ÿè®¡")
                node_stats_df = pd.DataFrame({
                    "èŠ‚ç‚¹ç±»å‹": list(stats.get("original_nodes", {}).keys()),
                    "åŸå§‹èŠ‚ç‚¹æ•°": list(stats.get("original_nodes", {}).values()),
                    "é‡‡æ ·èŠ‚ç‚¹æ•°": list(stats.get("sampled_nodes", {}).values()),
                })
                st.dataframe(node_stats_df, use_container_width=True)
                
                st.markdown("##### è¾¹ç»Ÿè®¡")
                edge_types = list(stats.get("original_edges", {}).keys())
                edge_stats_df = pd.DataFrame({
                    "è¾¹ç±»å‹": edge_types,
                    "åŸå§‹è¾¹æ•°": [stats.get("original_edges", {}).get(et, 0) for et in edge_types],
                    "é‡‡æ ·è¾¹æ•°": [stats.get("sampled_edges", {}).get(et, 0) for et in edge_types],
                })
                st.dataframe(edge_stats_df, use_container_width=True)
                
                st.markdown("##### åˆ†åŒºå¤§å°åˆ†å¸ƒ")
                partition_sizes = stats.get("partition_sizes", [])
                if partition_sizes:
                    partition_df = pd.DataFrame({
                        "åˆ†åŒº ID": list(range(len(partition_sizes))),
                        "èŠ‚ç‚¹æ•°": partition_sizes,
                    })
                    st.dataframe(partition_df, use_container_width=True)
            
            st.json(summary)

st.markdown("### ğŸ§¹ ç¼“å­˜ä¸è§„åˆ™ç»´æŠ¤")
col5, col6 = st.columns(2)
with col5:
    if st.button("åˆ·æ–°è§„åˆ™ç»“æ„", type="primary"):
        result = call_backend(
            "/api/v1/operations/rules/refresh",
            method="POST",
            spinner="æ­£åœ¨åˆ·æ–°è§„åˆ™ç»“æ„...",
            timeout=60,
        )
        if result is not None:
            st.success(result.get("message", "è§„åˆ™åˆ·æ–°å®Œæˆ"))
            _refresh_status_panel(status_placeholder)
with col6:
    if st.button("æ¸…ç©ºè§£é‡Šå™¨ç¼“å­˜", type="secondary"):
        result = call_backend(
            "/api/v1/operations/explainer/clear",
            method="POST",
            spinner="æ­£åœ¨æ¸…ç†è§£é‡Šå™¨ç¼“å­˜...",
            timeout=60,
        )
        if result is not None:
            st.success(result.get("message", "è§£é‡Šå™¨ç¼“å­˜å·²æ¸…ç©º"))
            _refresh_status_panel(status_placeholder)

st.markdown("### ğŸ›‘ åœæœºï¼ˆä»…æœ¬åœ°/å®¹å™¨è°ƒè¯•ï¼‰")
with st.form("shutdown_form"):
    st.warning("è¯¥æ“ä½œå°†æ˜¾å¼åœæ­¢åå°ä»»åŠ¡å¹¶å…³é—­åç«¯çš„å¼‚æ­¥äº‹ä»¶å¾ªç¯ï¼Œåœæœºåéœ€é‡å¯åç«¯è¿›ç¨‹ä»¥æ¢å¤æœåŠ¡ã€‚")
    confirm_shutdown = st.checkbox("æˆ‘å·²äº†è§£åœæœºå½±å“å¹¶ç¡®è®¤ç»§ç»­")
    submitted_shutdown = st.form_submit_button("æ‰§è¡Œåœæœº", type="primary")
    if submitted_shutdown:
        if not confirm_shutdown:
            st.warning("è¯·å‹¾é€‰ç¡®è®¤åå†æ‰§è¡Œåœæœºã€‚")
        else:
            result = call_backend(
                "/api/v1/operations/shutdown",
                method="POST",
                spinner="æ­£åœ¨åœæœº...",
                timeout=60,
            )
            if result is not None:
                st.success(result.get("message", "åç«¯åœæœºå®Œæˆ"))
                st.info("å¦‚éœ€ç»§ç»­ä½¿ç”¨ï¼Œè¯·é‡æ–°å¯åŠ¨åç«¯æœåŠ¡ã€‚")
                _refresh_status_panel(status_placeholder)

st.divider()
st.markdown("å¦‚éœ€æœ€æ–°çŠ¶æ€ï¼Œè¯·ä½¿ç”¨ä¾§è¾¹æ çš„é‡æ–°è¿è¡ŒæŒ‰é’®æˆ–åˆ·æ–°é¡µé¢ã€‚")
